{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import vmas\n",
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_agents = 5\n",
    "num_envs = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#simple spread\n",
    "env = vmas.make_env(\n",
    "    scenario=\"simple_spread\",\n",
    "    num_envs=num_envs,\n",
    "    n_agents=number_agents,\n",
    "    continuous_actions=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parser(obs:torch.Tensor):\n",
    "    #cur agents pos\n",
    "    cur_pos = obs[: ,0:2]\n",
    "    #print(\"cur_pos\", cur_pos, cur_pos.shape)\n",
    "    #cur agents vel\n",
    "    cur_vel = obs[: ,2:4]\n",
    "    #print(\"cur_vel\", cur_vel, cur_vel.shape)\n",
    "    #landmarks pos \n",
    "    landmarks = obs[:, 4:4 + 2 * number_agents]\n",
    "    #print(\"landmarks\", landmarks, landmarks.shape)\n",
    "    #other agents pos\n",
    "    other_agents = obs[:, 4 + 2 * number_agents:]\n",
    "    #print(\"other_agents\", other_agents, other_agents.shape)\n",
    "    return cur_pos, cur_vel, landmarks.contiguous().reshape(-1, number_agents, 2), other_agents.contiguous().reshape(-1, (number_agents - 1), 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgentPolicy(nn.Module):\n",
    "    def __init__(self, number_agents, agent_dim, landmark_dim, other_agent_dim):\n",
    "        super().__init__()\n",
    "        self.number_agents = number_agents\n",
    "\n",
    "\n",
    "        self.cur_agent_embedding = nn.Sequential(\n",
    "            nn.Linear(4, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "        self.landmark_embedding = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "        self.all_agent_embedding = nn.Sequential(\n",
    "            nn.Linear(2, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "\n",
    "        self.cross_attention = nn.MultiheadAttention(embed_dim=16, num_heads=1, batch_first=True)\n",
    "        self.processor = nn.Sequential(\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(16, 16)\n",
    "        )\n",
    "        self.self_attention = nn.MultiheadAttention(embed_dim=16, num_heads=1, batch_first=True)\n",
    "        self.mean_processor = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "        self.std_processor = nn.Sequential(\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 2)\n",
    "        )\n",
    "\n",
    "    def forward(self, obs, random_numbers):\n",
    "        cur_pos, cur_vel, landmarks, other_agents = parser(obs)\n",
    "        batch_size = cur_pos.shape[0]\n",
    "        print(\"Random numbers: \", random_numbers.shape)\n",
    "        cur_agent = torch.cat((cur_pos, cur_vel), dim=-1)\n",
    "        print(\"Current agent: \",cur_agent.shape)\n",
    "        all_agents_list = torch.cat((cur_pos.unsqueeze(1), other_agents), dim=1)\n",
    "        # print(\"All agents list: \", all_agents_list, all_agents_list.shape)\n",
    "\n",
    " \n",
    "        cur_agent_embeddings = self.cur_agent_embedding(cur_agent)\n",
    "        # print(\"Current agent embedding: \", cur_agent_embeddings, cur_agent_embeddings.shape)\n",
    "        landmark_embeddings = self.landmark_embedding(\n",
    "            landmarks.reshape(-1, 2)\n",
    "        ).reshape(-1, self.number_agents, 16)\n",
    "        print(\"Landmark embedding: \", landmark_embeddings.shape)\n",
    "\n",
    "        all_agents_embeddings = self.all_agent_embedding(\n",
    "            all_agents_list.reshape(-1, 2)  \n",
    "        ).reshape(-1, self.number_agents, 16)\n",
    "        print(\"All agents embedding: \", all_agents_embeddings.shape)\n",
    "\n",
    "        agents_mask = ~(random_numbers >= random_numbers[:, 0].view(-1,1))\n",
    "        attention_output, _ = self.cross_attention(\n",
    "            query=all_agents_embeddings,\n",
    "            key=landmark_embeddings,\n",
    "            value=landmark_embeddings,\n",
    "            attn_mask = agents_mask.unsqueeze(-2).repeat(1, self.number_agents, 1),\n",
    "            need_weights=False\n",
    "        )\n",
    "\n",
    "        attention_output = self.processor(attention_output)\n",
    "        attention_output = self.self_attention(attention_output, attention_output, attention_output, need_weights=False)[0].sum(dim=-2)\n",
    "        print(\"Attention output: \", attention_output, attention_output.shape)\n",
    "\n",
    "        latent = torch.concat((attention_output, cur_agent_embeddings), dim=-1)\n",
    "        mean = self.mean_processor(latent)\n",
    "        log_std = self.std_processor(latent)\n",
    "        print(\"Mean: \", mean, mean.shape)\n",
    "        print(\"Std: \", log_std, log_std.shape)\n",
    "        # Compute the action distribution\n",
    "\n",
    "        log_std = torch.clamp(log_std, min=-20, max=2)\n",
    "        log_std = log_std.exp()\n",
    "        \n",
    "        normal = torch.distributions.Normal(mean, log_std)\n",
    "        x_t = normal.rsample()\n",
    "        action = torch.tanh(x_t)\n",
    "        \n",
    "        log_prob = normal.log_prob(x_t) - torch.log((1 - action.pow(2)) + 1e-6)\n",
    "        log_prob = log_prob.sum(dim=1, keepdim=True)\n",
    "        \n",
    "        return action, log_prob\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels):\n",
    "        super().__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.hidden_channels = hidden_channels\n",
    "\n",
    "        layers = [nn.Linear(self.in_channels, self.hidden_channels[0]), nn.SiLU()]\n",
    "        for i in range(len(self.hidden_channels) - 1):\n",
    "            layers.append(nn.Linear(self.hidden_channels[i], self.hidden_channels[i + 1]))\n",
    "            if i < len(self.hidden_channels) - 2:\n",
    "                layers.append(nn.SiLU())\n",
    "        self.layers = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.layers(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RAP_qvalue(nn.Module):\n",
    "\n",
    "    def __init__(self, qvalue_config):\n",
    "        super().__init__()\n",
    "\n",
    "        self.device = qvalue_config[\"device\"]\n",
    "        self.na = qvalue_config[\"n_agents\"]\n",
    "        self.observation_dim_per_agent = qvalue_config[\"observation_dim_per_agent\"]\n",
    "        self.action_dim_per_agent = qvalue_config[\"action_dim_per_agent\"]\n",
    "\n",
    "        self.q1 = MLP(\n",
    "            in_channels=(self.observation_dim_per_agent + self.action_dim_per_agent) * self.na,\n",
    "            hidden_channels=[(self.observation_dim_per_agent + self.action_dim_per_agent) * 2 * self.na,\n",
    "                             (self.observation_dim_per_agent + self.action_dim_per_agent) * self.na,\n",
    "                             (self.observation_dim_per_agent + self.action_dim_per_agent),\n",
    "                             1]).to(self.device)\n",
    "        self.q2 = MLP(\n",
    "            in_channels=(self.observation_dim_per_agent + self.action_dim_per_agent) * self.na,\n",
    "            hidden_channels=[(self.observation_dim_per_agent + self.action_dim_per_agent) * 2 * self.na,\n",
    "                             (self.observation_dim_per_agent + self.action_dim_per_agent) * self.na,\n",
    "                             (self.observation_dim_per_agent + self.action_dim_per_agent),\n",
    "                             1]).to(self.device)\n",
    "\n",
    "    def forward(self, observation, action):\n",
    "        obs_action = torch.cat((observation.reshape([observation.shape[0], -1]),\n",
    "                                         action.reshape([action.shape[0], -1])), dim=1)\n",
    "        q1 = self.q1(obs_action)\n",
    "        q2 = self.q2(obs_action)\n",
    "        return q1, q2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = RandomAgentPolicy(number_agents, 4, 2 * number_agents, 2 * (number_agents - 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def permute(values, current_agent_idx):\n",
    "#     num_agents = len(values)\n",
    "#     other_agents = sorted([j for j in range(num_agents) if j != current_agent_idx])\n",
    "#     return torch.tensor([values[current_agent_idx]] + [values[j] for j in other_agents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_obs = env.reset()  \n",
    "# all_actions = []\n",
    "\n",
    "# env_random_numbers = torch.rand(num_envs, number_agents)\n",
    "\n",
    "# for j in range(num_envs):\n",
    "#     env_actions = []\n",
    "#     random_numbers = env_random_numbers[j]\n",
    "    \n",
    "#     # Loop over agents in this env\n",
    "#     for i in range(number_agents):\n",
    "#         obs = all_obs[i][j]\n",
    "#         permuted_numbers = permute(random_numbers, i)\n",
    "#         print(\"Permuted numbers: \", permuted_numbers)\n",
    "#         action = r(obs, permuted_numbers)\n",
    "#         env_actions.append(action)\n",
    "    \n",
    "#     all_actions.append(env_actions)\n",
    "\n",
    "# print(\"All actions:\", all_actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_permuted_env_random_numbers(env_random_numbers, number_agents, num_envs):\n",
    "    permutation_indices = torch.zeros(number_agents, number_agents, dtype=torch.long)\n",
    "    for i in range(number_agents):\n",
    "        other_agents = sorted([j for j in range(number_agents) if j != i])\n",
    "        permutation_indices[i] = torch.tensor([i] + other_agents)\n",
    "    expanded_rand = env_random_numbers.unsqueeze(1).expand(-1, number_agents, -1)\n",
    "    permuted_rand = torch.gather(\n",
    "        expanded_rand, \n",
    "        dim=2, \n",
    "        index=permutation_indices.unsqueeze(0).expand(num_envs, -1, -1)\n",
    "    )\n",
    "    return permuted_rand"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_random_numbers = torch.rand(num_envs, number_agents)\n",
    "permuted_env_random_numbers = get_permuted_env_random_numbers(env_random_numbers, number_agents, num_envs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "all_obs = env.reset()\n",
    "print(len(all_obs))\n",
    "obs_shape = all_obs[0][0].shape[0]\n",
    "obs_batched = torch.stack(all_obs, dim=1).reshape(-1, obs_shape)\n",
    "permuted_rand_batched = permuted_env_random_numbers.reshape(-1, number_agents)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 22])"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "obs_batched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 5])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "permuted_rand_batched.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random numbers:  torch.Size([15, 5])\n",
      "Current agent:  torch.Size([15, 4])\n",
      "Landmark embedding:  torch.Size([15, 5, 16])\n",
      "All agents embedding:  torch.Size([15, 5, 16])\n",
      "Attention output:  tensor([[-0.1968, -0.3158,  0.3873, -0.1054,  0.4220,  0.3282, -0.2650, -0.1924,\n",
      "          0.4498,  0.3498, -0.4717,  0.1989, -0.1826,  0.4444, -0.0900,  0.0515],\n",
      "        [-0.1890, -0.3253,  0.4149, -0.1328,  0.4250,  0.3330, -0.2787, -0.1933,\n",
      "          0.4620,  0.3469, -0.4887,  0.2120, -0.1953,  0.4646, -0.0706,  0.0580],\n",
      "        [-0.1810, -0.3324,  0.4359, -0.1484,  0.4257,  0.3432, -0.2865, -0.1978,\n",
      "          0.4727,  0.3487, -0.5079,  0.2201, -0.2030,  0.4823, -0.0603,  0.0675],\n",
      "        [-0.1849, -0.3379,  0.4399, -0.1564,  0.4303,  0.3368, -0.2879, -0.1992,\n",
      "          0.4693,  0.3438, -0.4974,  0.2218, -0.2078,  0.4742, -0.0524,  0.0617],\n",
      "        [-0.1957, -0.3224,  0.3991, -0.1188,  0.4250,  0.3288, -0.2686, -0.1932,\n",
      "          0.4543,  0.3492, -0.4751,  0.2060, -0.1886,  0.4499, -0.0794,  0.0524],\n",
      "        [-0.1962, -0.3378,  0.4078, -0.1289,  0.4291,  0.3344, -0.2729, -0.1907,\n",
      "          0.4616,  0.3550, -0.4781,  0.2104, -0.1916,  0.4616, -0.0697,  0.0535],\n",
      "        [-0.2074, -0.3396,  0.3909, -0.1127,  0.4316,  0.3314, -0.2617, -0.1977,\n",
      "          0.4508,  0.3581, -0.4584,  0.2044, -0.1850,  0.4404, -0.0787,  0.0478],\n",
      "        [-0.2039, -0.3533,  0.4034, -0.1241,  0.4347,  0.3397, -0.2699, -0.1936,\n",
      "          0.4613,  0.3640, -0.4697,  0.2092, -0.1882,  0.4595, -0.0692,  0.0519],\n",
      "        [-0.1895, -0.3195,  0.4057, -0.1252,  0.4215,  0.3305, -0.2760, -0.1901,\n",
      "          0.4590,  0.3462, -0.4848,  0.2070, -0.1918,  0.4611, -0.0771,  0.0561],\n",
      "        [-0.1986, -0.4062,  0.4216, -0.1509,  0.4383,  0.3649, -0.2884, -0.1666,\n",
      "          0.4932,  0.3885, -0.4863,  0.2114, -0.1934,  0.5201, -0.0444,  0.0561],\n",
      "        [-0.1989, -0.3728,  0.4205, -0.1456,  0.4366,  0.3464, -0.2821, -0.1850,\n",
      "          0.4743,  0.3666, -0.4781,  0.2134, -0.1971,  0.4835, -0.0523,  0.0535],\n",
      "        [-0.1960, -0.3402,  0.4093, -0.1308,  0.4296,  0.3353, -0.2742, -0.1898,\n",
      "          0.4630,  0.3558, -0.4789,  0.2110, -0.1921,  0.4643, -0.0681,  0.0537],\n",
      "        [-0.1885, -0.3381,  0.4299, -0.1494,  0.4293,  0.3349, -0.2836, -0.1965,\n",
      "          0.4663,  0.3456, -0.4892,  0.2178, -0.2041,  0.4693, -0.0567,  0.0583],\n",
      "        [-0.1842, -0.3193,  0.4065, -0.1208,  0.4183,  0.3392, -0.2830, -0.1880,\n",
      "          0.4645,  0.3498, -0.4980,  0.2039, -0.1892,  0.4760, -0.0821,  0.0630],\n",
      "        [-0.1966, -0.3360,  0.4052, -0.1284,  0.4281,  0.3315, -0.2716, -0.1891,\n",
      "          0.4597,  0.3529, -0.4741,  0.2085, -0.1918,  0.4583, -0.0702,  0.0511]],\n",
      "       grad_fn=<SumBackward1>) torch.Size([15, 16])\n",
      "Mean:  tensor([[ 0.0563, -0.0843],\n",
      "        [ 0.0550, -0.0890],\n",
      "        [ 0.0542, -0.1005],\n",
      "        [ 0.0589, -0.1032],\n",
      "        [ 0.0566, -0.0858],\n",
      "        [ 0.0522, -0.0925],\n",
      "        [ 0.0590, -0.0810],\n",
      "        [ 0.0578, -0.0838],\n",
      "        [ 0.0518, -0.0920],\n",
      "        [ 0.0613, -0.0936],\n",
      "        [ 0.0625, -0.1022],\n",
      "        [ 0.0601, -0.0940],\n",
      "        [ 0.0572, -0.1002],\n",
      "        [ 0.0515, -0.0922],\n",
      "        [ 0.0587, -0.0853]], grad_fn=<AddmmBackward0>) torch.Size([15, 2])\n",
      "Std:  tensor([[-0.0735,  0.1483],\n",
      "        [-0.0747,  0.1463],\n",
      "        [-0.0691,  0.1577],\n",
      "        [-0.0745,  0.1510],\n",
      "        [-0.0741,  0.1475],\n",
      "        [-0.0717,  0.1507],\n",
      "        [-0.0713,  0.1512],\n",
      "        [-0.0746,  0.1476],\n",
      "        [-0.0625,  0.1585],\n",
      "        [-0.0816,  0.1356],\n",
      "        [-0.0781,  0.1439],\n",
      "        [-0.0796,  0.1407],\n",
      "        [-0.0754,  0.1489],\n",
      "        [-0.0716,  0.1489],\n",
      "        [-0.0777,  0.1421]], grad_fn=<AddmmBackward0>) torch.Size([15, 2])\n"
     ]
    }
   ],
   "source": [
    "actions_batched, log_probs_batched = r(obs_batched, permuted_rand_batched)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q-values: tensor([[0.0092],\n",
      "        [0.0105],\n",
      "        [0.0070]], grad_fn=<AddmmBackward0>) tensor([[0.2030],\n",
      "        [0.1906],\n",
      "        [0.2064]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "obs_grouped = obs_batched.reshape(num_envs, number_agents, -1)\n",
    "actions_grouped = actions_batched.reshape(num_envs, number_agents, -1)\n",
    "\n",
    "obs_flat = obs_grouped.reshape(num_envs, -1)\n",
    "actions_flat = actions_grouped.reshape(num_envs, -1)\n",
    "\n",
    "qvalue_config = {\n",
    "    \"device\": \"cpu\",\n",
    "    \"n_agents\": number_agents,\n",
    "    \"observation_dim_per_agent\": obs_batched.shape[-1],\n",
    "    \"action_dim_per_agent\": actions_batched.shape[-1]   \n",
    "}\n",
    "q_network = RAP_qvalue(qvalue_config)\n",
    "\n",
    "# Compute Q-values\n",
    "q_1, q_2 = q_network(obs_flat, actions_flat)\n",
    "print(\"Q-values:\", q_1, q_2)\n",
    "# print(\"Q-values shape:\", q_values.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity, device=\"cpu\"):\n",
    "        self.capacity = capacity\n",
    "        self.device = device\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "\n",
    "    def add(self, obs, action, reward, next_obs, done, random_numbers):\n",
    "        obs = torch.as_tensor(obs, device=self.device)\n",
    "        action = torch.as_tensor(action, device=self.device)\n",
    "        reward = torch.as_tensor(reward, device=self.device)\n",
    "        next_obs = torch.as_tensor(next_obs, device=self.device)\n",
    "        done = torch.as_tensor(done, device=self.device)\n",
    "        random_numbers = torch.as_tensor(random_numbers, device=self.device)\n",
    "\n",
    "        for i in range(obs.shape[0]):  # Loop over envs\n",
    "            self.buffer.append((\n",
    "                obs[i], action[i], reward[i], next_obs[i], done[i], random_numbers[i]\n",
    "            ))\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        obs, actions, rewards, next_obs, dones, rand_nums = zip(*batch)\n",
    "        return (\n",
    "            torch.stack(obs),\n",
    "            torch.stack(actions),\n",
    "            torch.stack(rewards),\n",
    "            torch.stack(next_obs),\n",
    "            torch.stack(dones),\n",
    "            torch.stack(rand_nums)\n",
    "        )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
